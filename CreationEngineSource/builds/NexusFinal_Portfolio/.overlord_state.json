{
  "files": {
    "main.py": "{settings.API_V1_STR}/openapi.json\",\n    docs_url=f\"{settings.API_V1_STR}/docs\",\n    redoc_url=f\"{settings.API_V1_STR}/redoc\",\n)\n\n# Add custom middleware for CORS, error handling, etc.\nadd_middleware(app)\n\n# Mount static files directory\napp.mount(\"/static\", StaticFiles(directory=\"frontend/static\"), name=\"static\")\n\n# Configure Jinja2 templates\ntemplates = Jinja2Templates(directory=\"frontend/templates\")\n\n# Global instances for background safety services\nhealth_monitor: Optional[HealthMonitor] = None\nwatchdog: Optional[Watchdog] = None\nbackup_service: Optional[BackupService] = None\n\nasync def seed_initial_data(db: AsyncSession):\n    \"\"\"\n    Seeds initial data into the database if it's empty.\n    This includes a superuser, a regular user, and some demo portfolio data.\n    \"\"\"\n    try:\n        # Check if any user exists to prevent re-seeding\n        existing_user = await crud_user.get_user_by_email(db, email=settings.FIRST_SUPERUSER_EMAIL)\n        if not existing_user:\n            logger.info(\"Seeding initial superuser...\")\n            user_in = UserCreate(\n                email=settings.FIRST_SUPERUSER_EMAIL,\n                password=settings.FIRST_SUPERUSER_PASSWORD,\n                full_name=\"Admin User\",\n                is_superuser=True,\n                is_active=True,\n            )\n            await crud_user.create_user(db, user_in=user_in)\n            logger.info(\"Superuser seeded successfully.\")\n\n            # Create a regular user for demo purposes\n            logger.info(\"Seeding initial regular user...\")\n            regular_user_in = UserCreate(\n                email=\"demo_user@app.com\", # Using a generic internal-looking email\n                password=\"securepassword\",\n                full_name=\"Demo User\",\n                is_superuser=False,\n                is_active=True,\n            )\n            await crud_user.create_user(db, user_in=regular_user_in)\n            logger.info(\"Regular user seeded successfully.\")\n\n            # Retrieve the demo user to link assets/transactions\n            demo_user = await crud_user.get_user_by_email(db, email=\"demo_user@app.com\")\n            if demo_user:\n                logger.info(\"Seeding initial demo assets and transactions for demo user...\")\n                # Create some demo assets\n                asset1_in = AssetCreate(\n                    symbol=\"AAPL\",\n                    name=\"Apple Inc.\",\n                    asset_type=\"Stock\",\n                    quantity=10.5,\n                    purchase_price=150.00,\n                    purchase_date=\"2023-01-15\",\n                    user_id=demo_user.id\n                )\n                asset1 = await crud_asset.create_asset(db, asset_in=asset1_in)\n\n                asset2_in = AssetCreate(\n                    symbol=\"GOOGL\",\n                    name=\"Alphabet Inc. (Class A)\",\n                    asset_type=\"Stock\",\n                    quantity=5.0,\n                    purchase_price=100.00,\n                    purchase_date=\"2023-03-01\",\n                    user_id=demo_user.id\n                )\n                asset2 = await crud_asset.create_asset(db, asset_in=asset2_in)\n\n                # Create some demo transactions\n                transaction1_in = TransactionCreate(\n                    asset_id=asset1.id,\n                    transaction_type=\"BUY\",\n                    quantity=10.5,\n                    price=150.00,\n                    transaction_date=\"2023-01-15\",\n                    user_id=demo_user.id\n                )\n                await crud_transaction.create_transaction(db, transaction_in=transaction1_in)\n\n                transaction2_in = TransactionCreate(\n                    asset_id=asset2.id,\n                    transaction_type=\"BUY\",\n                    quantity=5.0,\n                    price=100.00,\n                    transaction_date=\"2023-03-01\",\n                    user_id=demo_user.id\n                )\n                await crud_transaction.create_transaction(db, transaction_in=transaction2_in)\n\n                # Create a demo watchlist item\n                watchlist_in = WatchlistCreate(\n                    symbol=\"MSFT\",\n                    name=\"Microsoft Corp.\",\n                    user_id=demo_user.id\n                )\n                await crud_watchlist.create_watchlist_item(db, watchlist_in=watchlist_in)\n\n                logger.info(\"Demo assets, transactions, and watchlist seeded successfully.\")\n            else:\n                logger.warning(\"Demo user not found after creation, skipping demo asset/transaction seeding.\")\n        else:\n            logger.info(\"Superuser already exists, skipping initial data seeding.\")\n\n    except Exception as e:\n        logger.error(f\"Error seeding initial data: {e}\", exc_info=True)\n\n\n@app.on_event(\"startup\")\nasync def startup_event():\n    \"\"\"\n    Handles application startup events:\n    - Initializes the database connection.\n    - Starts background health monitoring and watchdog services.\n    - Initializes and starts the backup service.\n    - Performs initial data seeding if enabled.\n    \"\"\"\n    global health_monitor, watchdog, backup_service\n    logger.info(f\"\ud83d\ude80 Starting {settings.PROJECT_NAME} v{settings.VERSION}...\")\n    logger.info(f\"Environment: {settings.ENVIRONMENT}\")\n    # Mask sensitive parts of the DB URL for logging\n    db_url_display = settings.DATABASE_URL.split('@')[-1] if '@' in settings.DATABASE_URL else settings.DATABASE_URL\n    logger.info(f\"Database URL: {db_url_display}\")\n\n    try:\n        await init_db()\n        logger.info(\"Database initialized successfully.\")\n    except Exception as e:\n        logger.critical(f\"Failed to initialize database: {e}. Application may not function correctly.\", exc_info=True)\n        # In a production environment, you might want to raise an exception or exit here.\n\n    # Ensure backup directory exists\n    os.makedirs(settings.BACKUP_DIR, exist_ok=True)\n\n    # Initialize and start HealthMonitor in a background task\n    health_monitor = HealthMonitor(\n        db_url=settings.DATABASE_URL,\n        check_interval_seconds=settings.HEALTH_CHECK_INTERVAL_SECONDS\n    )\n    asyncio.create_task(health_monitor.start())\n    logger.info(\"HealthMonitor started.\")\n\n    # Initialize and start Watchdog in a background task\n    watchdog = Watchdog(\n        disk_threshold_percent=settings.WATCHDOG_DISK_THRESHOLD_PERCENT,\n        memory_threshold_percent=settings.WATCHDOG_MEMORY_THRESHOLD_PERCENT,\n        check_interval_seconds=settings.WATCHDOG_CHECK_INTERVAL_SECONDS\n    )\n    asyncio.create_task(watchdog.start())\n    logger.info(\"Watchdog started.\")\n\n    # Initialize BackupService and start periodic backups\n    backup_service = BackupService(\n        backup_dir=settings.BACKUP_DIR,\n        db_url=settings.DATABASE_URL,\n        backup_interval_seconds=settings.BACKUP_INTERVAL_SECONDS\n    )\n    asyncio.create_task(backup_service.start_periodic_backup())\n    logger.info(\"BackupService initialized and periodic backup started.\")\n\n    # Seed initial data if enabled in settings\n    if settings.SEED_DATABASE:\n        logger.info(\"Attempting to seed initial data...\")\n        # Acquire a database session for seeding\n        async for db_session in get_db():\n            await seed_initial_data(db_session)\n            break # Only need one session for this one-off task\n    else:\n        logger.info(\"Database seeding is disabled by configuration.\")\n\n    logger.info(f\"\u2705 {settings.PROJECT_NAME} startup complete.\")\n\n\n@app.on_event(\"shutdown\")\nasync def shutdown_event():\n    \"\"\"\n    Handles application shutdown events:\n    - Stops all background safety services.\n    - Performs a final database backup.\n    \"\"\"\n    global health_monitor, watchdog, backup_service\n    logger.info(f\"Shutting down {settings.PROJECT_NAME}...\")\n\n    # Stop HealthMonitor gracefully\n    if health_monitor:\n        await health_monitor.stop()\n        logger.info(\"HealthMonitor stopped.\")\n    \n    # Stop Watchdog gracefully\n    if watchdog:\n        await watchdog.stop()\n        logger.info(\"Watchdog stopped.\")\n    \n    # Stop periodic backups and perform a final backup\n    if backup_service:\n        await backup_service.stop_periodic_backup()\n        logger.info(\"BackupService periodic backup stopped.\")\n        try:\n            await backup_service.perform_backup()\n            logger.info(\"Final backup performed successfully on shutdown.\")\n        except Exception as e:\n            logger.error(f\"Failed to perform final backup on shutdown: {e}\", exc_info=True)\n\n    logger.info(f\"\ud83d\udc4b {settings.PROJECT_NAME} shutdown complete.\")\n\n\n# Include API routers under the specified API prefix\napp.include_router(api_router, prefix=settings.API_V1_STR)\n\n\n@app.get(\"/\", response_class=HTMLResponse, name=\"root\")\nasync def read_root(request: Request):\n    \"\"\"\n    Serves the main landing page of the application.\n    The frontend JavaScript will handle redirection to login/dashboard based on authentication status.\n    \"\"\"\n    return templates.TemplateResponse(\"index.html\", {\"request\": request, \"project_name\": settings.PROJECT_NAME})\n\n@app.get(\"/login\", response_class=HTMLResponse, name=\"login\")\nasync def login_page(request: Request):\n    \"\"\"\n    Serves the user login page.\n    \"\"\"\n    return templates.TemplateResponse(\"auth/login.html\", {\"request\": request, \"project_name\": settings.PROJECT_NAME})\n\n@app.get(\"/register\", response_class=HTMLResponse, name=\"register\")\nasync def register_page(request: Request):\n    \"\"\"\n    Serves the user registration page.\n    \"\"\"\n    return templates.TemplateResponse(\"auth/register.html\", {\"request\": request, \"project_name\": settings.PROJECT_NAME})\n\n@app.get(\"/dashboard\", response_class=HTMLResponse, name=\"dashboard\")\nasync def dashboard_page(request: Request):\n    \"\"\"\n    Serves the user dashboard page.\n    In a production setup, this route would typically be protected by authentication.\n    \"\"\"\n    return templates.TemplateResponse(\"dashboard/index.html\", {\"request\": request, \"project_name\": settings.PROJECT_NAME})\n\n@app.get(\"/portfolio\", response_class=HTMLResponse, name=\"portfolio\")\nasync def portfolio_page(request: Request):\n    \"\"\"\n    Serves the user's portfolio management page.\n    \"\"\"\n    return templates.TemplateResponse(\"portfolio/index.html\", {\"request\": request, \"project_name\": settings.PROJECT_NAME})\n\n@app.get(\"/portfolio/add\", response_class=HTMLResponse, name=\"add_asset\")\nasync def add_asset_page(request: Request):\n    \"\"\"\n    Serves the page for adding a new asset to the portfolio.\n    \"\"\"\n    return templates.TemplateResponse(\"portfolio/add_asset.html\", {\"request\": request, \"project_name\": settings.PROJECT_NAME})\n\n@app.get(\"/portfolio/edit/{asset_id}\", response_class=HTMLResponse, name=\"edit_asset\")\nasync def edit_asset_page(request: Request, asset_id: int):\n    \"\"\"\n    Serves the page for editing an existing portfolio asset.\n    The `asset_id` is passed to the template for dynamic content loading.\n    \"\"\"\n    return templates.TemplateResponse(\"portfolio/edit_asset.html\", {\"request\": request, \"project_name\": settings.PROJECT_NAME, \"asset_id\": asset_id})\n\n@app.get(\"/transactions\", response_class=HTMLResponse, name=\"transactions\")\nasync def transactions_page(request: Request):\n    \"\"\"\n    Serves the page displaying the user's transaction history.\n    \"\"\"\n    return templates.TemplateResponse(\"transactions/index.html\", {\"request\": request, \"project_name\": settings.PROJECT_NAME})\n\n@app.get(\"/watchlist\", response_class=HTMLResponse, name=\"watchlist\")\nasync def watchlist_page(request: Request):\n    \"\"\"\n    Serves the user's watchlist page.\n    \"\"\"\n    return templates.TemplateResponse(\"watchlist/index.html\", {\"request\": request, \"project_name\": settings.PROJECT_NAME})\n\n@app.get(\"/profile\", response_class=HTMLResponse, name=\"profile\")\nasync def profile_page(request: Request):\n    \"\"\"\n    Serves the user's profile viewing page.\n    \"\"\"\n    return templates.TemplateResponse(\"user/profile.html\", {\"request\": request, \"project_name\": settings.PROJECT_NAME})\n\n@app.get(\"/settings\", response_class=HTMLResponse, name=\"settings\")\nasync def settings_page(request: Request):\n    \"\"\"\n    Serves the user's account settings page.\n    \"\"\"\n    return templates.TemplateResponse(\"user/settings.html\", {\"request\": request, \"project_name\": settings.PROJECT_NAME}",
    ".dockerignore": ".git\n.gitignore\n.env\n.vscode/\n.idea/\n__pycache__/\n*.pyc\n*.pyo\n*.pyd\nvenv/\n.venv/\n.pytest_cache/\n.mypy_cache/\n*.egg-info/\nbuild/\ndist/\n*.log\nlogs/\nalembic/versions/*.pyc\nalembic/versions/__pycache__/\nnpm-debug.log*\nyarn-debug.log*\nyarn-error.log*\nnode_modules/\nfrontend/static/css/tailwind.css.map # If generated\nfrontend/static/js/*.map # If generated\ntmp/\ntemp/",
    ".env.example": "SERVER_NAME=\"localhost:8000\"\nAPI_V1_STR=\"/api/v1\"\n\n# Database Configuration\nDATABASE_URL=\"postgresql+asyncpg://user:password@db:5432/mydatabase\"\n# For local development with SQLite:\n# DATABASE_URL=\"sqlite+aiosqlite:///./sql_app.db\"\n\n# JWT Authentication\nSECRET_KEY=\"YOUR_SUPER_SECRET_KEY_HERE_CHANGE_ME_IN_PRODUCTION\"\nALGORITHM=\"HS256\"\nACCESS_TOKEN_EXPIRE_MINUTES=30\nREFRESH_TOKEN_EXPIRE_DAYS=7\n\n# Admin User (for initial setup or testing)\nFIRST_SUPERUSER_EMAIL=\"admin@example.com\"\nFIRST_SUPERUSER_PASSWORD=\"securepassword\"\n\n# Market Data API (e.g., Alpha Vantage, Finnhub, Twelve Data)\n# Replace with your chosen provider and API key\nMARKET_DATA_API_KEY=\"YOUR_MARKET_DATA_API_KEY\"\nMARKET_DATA_BASE_URL=\"https://api.examplemarketdata.com/v1\" # Example URL, replace with actual\nMARKET_DATA_PROVIDER=\"ExampleMarketData\" # e.g., \"AlphaVantage\", \"Finnhub\"\n\n# Logging Configuration\nLOG_LEVEL=\"INFO\" # Options: DEBUG, INFO, WARNING, ERROR, CRITICAL\nLOG_FILE_PATH=\"./logs/app.log\"\nLOG_RETENTION_DAYS=7\n\n# CORS Configuration\nBACKEND_CORS_ORIGINS='[\"http://localhost:3000\", \"http://localhost:8000\"]' # JSON array of origins\n\n# Watchdog/Health Monitoring\nWATCHDOG_INTERVAL_SECONDS=60\nDISK_USAGE_THRESHOLD_PERCENT=90\nMEMORY_USAGE_THRESHOLD_PERCENT=85\n\n# Backup/Restore Configuration\nBACKUP_DIR=\"./backups\"\nBACKUP_RETENTION_DAYS=30\n\n# Frontend Configuration (if served by backend)\nTEMPLATES_DIR=\"frontend/templates\"\nSTATIC_DIR=\"frontend/static\"\n\n# Environment (e.g., \"development\", \"production\", \"testing\")\nENVIRONMENT=\"development\"",
    ".gitignore": "# Python\n__pycache__/\n*.pyc\n*.pyo\n*.pyd\n.pytest_cache/\n.mypy_cache/\n.venv/\nvenv/\n\n# IDEs\n.vscode/\n.idea/\n*.iml\n\n# Environment variables\n.env\n.env.*\n!.env.example # Keep the example file\n\n# Logs\n*.log\nlogs/\n\n# Database\n*.db # For SQLite databases, e.g., sql_app.db\n*.sqlite\n*.sqlite3\n\n# Alembic\nalembic/versions/*.pyc\nalembic/versions/__pycache__/\n\n# Build artifacts\nbuild/\ndist/\n*.egg-info/\n.coverage\n.tox/\n\n# Frontend specific (if applicable, e.g., node_modules for JS frontend)\nnode_modules/\nnpm-debug.log*\nyarn-debug.log*\nyarn-error.log*\nfrontend/static/css/tailwind.css.map # If generated by Tailwind JIT/CLI\nfrontend/static/js/*.map # If generated by JS bundlers\n\n# Backups and temporary files\nbackups/ # As per BACKUP_DIR in .env.example\ntmp/\ntemp/\n*.swp\n*~\n.#*",
    "Dockerfile": "FROM python:3.12-slim-bookworm\n\n# Set environment variables\nENV PYTHONUNBUFFERED 1\nENV PYTHONDONTWRITEBYTECODE 1\nENV APP_HOME /app\n\n# Create app directory\nWORKDIR $APP_HOME\n\n# Install system dependencies\n# We need build-essential for some python packages (e.g., psycopg2-binary)\n# and libpq-dev for PostgreSQL client libraries.\n# curl is useful for health checks.\nRUN apt-get update \\\n    && apt-get install -y --no-install-recommends \\\n        build-essential \\\n        libpq-dev \\\n        curl \\\n    && rm -rf /var/lib/apt/lists/*\n\n# Copy requirements file and install Python dependencies\n# This step is separated to leverage Docker's layer caching.\n# If requirements.txt doesn't change, this layer won't be rebuilt.\nCOPY requirements.txt .\nRUN pip install --no-cache-dir -r requirements.txt\n\n# Copy the rest of the application code\nCOPY . $APP_HOME\n\n# Expose the port FastAPI will run on\nEXPOSE 8000\n\n# Command to run the application\n# Using Uvicorn with Gunicorn for production-ready deployment.\n# --host 0.0.0.0 makes the server accessible from outside the container.\n# --workers can be adjusted based on CPU cores (2-4 * CPU_CORES + 1 is a common heuristic).\n# --bind 0.0.0.0:8000 specifies the address and port for Gunicorn.\n# main:app refers to the FastAPI application instance named 'app' in 'main.py'.\n# --worker-class uvicorn.workers.UvicornWorker ensures Uvicorn handles ASGI.\nCMD [\"gunicorn\", \"main:app\", \"--workers\", \"4\", \"--worker-class\", \"uvicorn.workers.UvicornWorker\", \"--bind\", \"0.0.0.0:8000\"]",
    "README.md": ".\n\u251c\u2500\u2500 .dockerignore\n\u251c\u2500\u2500 .env.example             # Example environment variables\n\u251c\u2500\u2500 .gitignore\n\u251c\u2500\u2500 Dockerfile               # Docker build instructions\n\u251c\u2500\u2500 README.md                # This file\n\u251c\u2500\u2500 alembic.ini              # Alembic configuration\n\u251c\u2500\u2500 alembic/                 # Database migration scripts\n\u2502   \u251c\u2500\u2500 env.py\n\u2502   \u251c\u2500\u2500 script.py.mako\n\u2502   \u2514\u2500\u2500 versions/\n\u251c\u2500\u2500 backend/                 # FastAPI backend application\n\u2502   \u251c\u2500\u2500 api/\n\u2502   \u2502   \u2514\u2500\u2500 v1/              # API version 1\n\u2502   \u2502       \u251c\u2500\u2500 api.py       # Main API router\n\u2502   \u2502       \u2514\u2500\u2500 endpoints/   # API endpoints\n\u2502   \u2502           \u251c\u2500\u2500 auth.py\n\u2502   \u2502           \u251c\u2500\u2500 dashboard.py\n\u2502   \u2502           \u251c\u2500\u2500 health.py\n\u2502   \u2502           \u251c\u2500\u2500 market.py\n\u2502   \u2502           \u251c\u2500\u2500 portfolio.py\n\u2502   \u2502           \u251c\u2500\u2500 transactions.py\n\u2502   \u2502           \u251c\u2500\u2500 users.py\n\u2502   \u2502           \u2514\u2500\u2500 watchlist.py\n\u2502   \u251c\u2500\u2500 config.py            # Application settings\n\u2502   \u251c\u2500\u2500 core/                # Core utilities and services\n\u2502   \u2502   \u251c\u2500\u2500 exceptions.py\n\u2502   \u2502   \u251c\u2500\u2500 logging_config.py\n\u2502   \u2502   \u2514\u2500\u2500 middleware.py\n\u2502   \u251c\u2500\u2500 crud.py              # CRUD operations for database models\n\u2502   \u251c\u2500\u2500 database.py          # Database session and engine setup\n\u2502   \u251c\u2500\u2500 dependencies.py      # Dependency injection for FastAPI\n\u2502   \u251c\u2500\u2500 models.py            # SQLAlchemy database models\n\u2502   \u251c\u2500\u2500 schemas.py           # Pydantic schemas for request/response validation\n\u2502   \u251c\u2500\u2500 security.py          # Authentication and authorization logic\n\u2502   \u2514\u2500\u2500 services/            # Background services and external integrations\n\u2502       \u251c\u2500\u2500 backup_restore.py\n\u2502       \u251c\u2500\u2500 health_monitor.py\n\u2502       \u251c\u2500\u2500 market_data.py\n\u2502       \u251c\u2500\u2500 retry_logic.py\n\u2502       \u2514\u2500\u2500 watchdog.py\n\u251c\u2500\u2500 frontend/                # Frontend assets and templates\n\u2502   \u251c\u2500\u2500 static/\n\u2502   \u2502   \u251c\u2500\u2500 css/\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 tailwind.css\n\u2502   \u2502   \u2514\u2500\u2500 js/\n\u2502   \u2502       \u251c\u2500\u2500 alpine.min.js\n\u2502   \u2502       \u251c\u2500\u2500 auth.js\n\u2502   \u2502       \u251c\u2500\u2500 chart.min.js\n\u2502   \u2502       \u251c\u2500\u2500 dashboard.js\n\u2502   \u2502       \u251c\u2500\u2500 main.js\n\u2502   \u2502       \u251c\u2500\u2500 portfolio.js\n\u2502   \u2502       \u251c\u2500\u2500 transactions.js\n\u2502   \u2502       \u251c\u2500\u2500 user.js\n\u2502   \u2502       \u2514\u2500\u2500 watchlist.js\n\u2502   \u2514\u2500\u2500 templates/\n\u2502       \u251c\u2500\u2500 auth/\n\u2502       \u2502   \u251c\u2500\u2500 login.html\n\u2502       \u2502   \u2514\u2500\u2500 register.html\n\u2502       \u251c\u2500\u2500 base.html\n\u2502       \u251c\u2500\u2500 dashboard/\n\u2502       \u2502   \u2514\u2500\u2500 index.html\n\u2502       \u251c\u2500\u2500 index.html\n\u2502       \u251c\u2500\u2500 portfolio/\n\u2502       \u2502   \u251c\u2500\u2500 add_asset.html\n\u2502       \u2502   \u251c\u2500\u2500 edit_asset.html\n\u2502       \u2502   \u2514\u2500\u2500 index.html\n\u2502       \u251c\u2500\u2500 transactions/\n\u2502       \u2502   \u2514\u2500\u2500 index.html\n\u2502       \u251c\u2500\u2500 user/\n\u2502       \u2502   \u251c\u2500\u2500 profile.html\n\u2502       \u2502   \u2514\u2500\u2500 settings.html\n\u2502       \u2514\u2500\u2500 watchlist/\n\u2502           \u2514\u2500\u2500 index.html\n\u251c\u2500\u2500 main.py                  # Main FastAPI application entry point\n\u2514\u2500\u2500 requirements.txt         # Python dependencies",
    "alembic.ini": "[alembic]\n# This is the main configuration file for Alembic.\n#\n# For more information, see http://alembic.sqlalchemy.org/en/latest/\n#\n# The script directory (where alembic stores migration scripts)\nscript_location = alembic\n\n# The file used to store the current revision.\n# This file is not typically committed to version control.\n# It's used by Alembic to track the database state.\nversion_locations = alembic/versions\n\n# A generic, placeholder URL for the database.\n# This value is typically overridden by the `env.py` script,\n# which loads the actual DATABASE_URL from the application's\n# configuration (e.g., from environment variables or a .env file).\n# This ensures that Alembic uses the same database configuration\n# as the FastAPI application.\nsqlalchemy.url = postgresql+asyncpg://user:password@db:5432/mydatabase\n\n# The name of the database table used to store Alembic's revision history.\n# This table will be created automatically by Alembic if it doesn't exist.\nalembic_version_table = alembic_version\n\n# The template used to generate new migration scripts.\n# This uses Mako syntax.\nfile_template = %%(rev)s_%%(slug)s\n\n# Set to 'true' to have Alembic generate a 'downgrade' function\n# in new migration scripts, even if it's empty.\n# This is useful for ensuring that every upgrade has a corresponding downgrade path.\nautogenerate = false\n\n# Logging configuration\n# This section configures how Alembic logs its operations.\n# It uses Python's standard logging module.\n[loggers]\nkeys = root, alembic, sqlalchemy\n\n[handlers]\nkeys = console\n\n[formatters]\nkeys = generic\n\n[logger_root]\nlevel = WARNING\nhandlers = console\nqualname =\n\n[logger_alembic]\nlevel = INFO\nhandlers = console\nqualname = alembic\n\n[logger_sqlalchemy]\nlevel = WARNING\nhandlers = console\nqualname = sqlalchemy.engine\n\n[handler_console]\nclass = StreamHandler\nformatter = generic\nargs = (sys.stderr,)\n\n[formatter_generic]\nformat = %(levelname)-5.5s [%(name)s] %(message)s\ndatefmt = %H:%M:%S",
    "alembic/env.py": "{e}. Ensure backend directory is in PYTHONPATH.\")\n    sys.exit(1)\n\n# Setup logging for Alembic\nsetup_logging()\nlogger = get_logger(__name__)\n\n# this is the Alembic Config object, which provides\n# access to values within the .ini file in use.\nconfig = context.config\n\n# Interpret the config file for Python's standard logging.\n# This uses the logging configuration from alembic.ini.\nfileConfig(config.config_file_name)\n\n# Target metadata for 'autogenerate' support.\n# This tells Alembic which SQLAlchemy models to track for schema changes.\ntarget_metadata = Base.metadata\n\n\ndef run_migrations_offline() -> None:\n    \"\"\"Run migrations in 'offline' mode.\n\n    This configures the context with just a URL and not an Engine.\n    The metadata is made available to the context as a non-db specific object.\n    When completing an offline migration, the output is written to a file.\n    \"\"\"\n    # Get the database URL from alembic.ini, but prioritize application settings\n    url = config.get_main_option(\"sqlalchemy.url\")\n    if settings.DATABASE_URL:\n        url = settings.DATABASE_URL\n        logger.info(f\"Using DATABASE_URL from settings for offline migrations: {url}\")\n    else:\n        logger.warning(\"DATABASE_URL not found in settings, falling back to alembic.ini URL for offline migrations.\")\n\n    context.configure(\n        url=url,\n        target_metadata=target_metadata,\n        literal_binds=True,\n        dialect_opts={\"paramstyle\": \"named\"},\n        compare_type=True, # Enable type comparison for autogenerate\n    )\n\n    with context.begin_transaction():\n        context.run_migrations()\n\n\ndef do_run_migrations(connection) -> None:\n    \"\"\"Helper function to run migrations within a given connection.\"\"\"\n    context.configure(\n        connection=connection,\n        target_metadata=target_metadata,\n        compare_type=True, # Enable type comparison for autogenerate\n    )\n\n    with context.begin_transaction():\n        context.run_migrations()\n\n\nasync def run_migrations_online() -> None:\n    \"\"\"Run migrations in 'online' mode.\n\n    In this scenario, we need to create an AsyncEngine\n    and associate an asynchronous connection with the context.\n    \"\"\"\n    # Get the database URL from alembic.ini, but prioritize application settings\n    connectable_url = config.get_main_option(\"sqlalchemy.url\")\n    if settings.DATABASE_URL:\n        connectable_url = settings.DATABASE_URL\n        logger.info(f\"Using DATABASE_URL from settings for online migrations: {connectable_url}\")\n    else:\n        logger.error(\"DATABASE_URL not found in settings. Cannot run online migrations without a database URL.\")\n        raise ValueError(\"DATABASE_URL is not configured for online migrations.\")\n\n    # Create an async engine using the configured URL\n    # NullPool is often used for Alembic to avoid connection pooling issues during schema changes.\n    engine = create_async_engine(\n        connectable_url,\n        poolclass=pool.NullPool,\n        future=True # Use SQLAlchemy 2.0 style\n    )\n\n    try:\n        async with engine.connect() as connection:\n            # Run synchronous migration operations within the async connection\n            await connection.run_sync(do_run_migrations)\n    except Exception as e:\n        logger.critical(f\"Error during online migrations: {e}\", exc_info=True)\n        raise # Re-raise to ensure the script exits with an error\n    finally:\n        # Ensure the engine is properly disposed after migrations\n        await engine.dispose()\n\n\nif context.is_offline_mode():\n    run_migrations_offline()\nelse:\n    try:\n        asyncio.run(run_migrations_online())\n    except Exception as e:\n        logger.critical(f\"Alembic migration failed: {e}",
    "alembic/script.py.mako": "{encoding}\n\"\"\"${message}\n\nRevision ID: ${up_revision}\nRevises: ${down_revision}\nCreate Date: ${create_date}\n\n\"\"\"\nfrom alembic import op\nimport sqlalchemy as sa\n\n\n# revision identifiers, used by Alembic.\nrevision = '${up_revision}'\ndown_revision = '${down_revision}'\nbranch_labels = ${repr(branch_labels)}\ndepends_on = ${repr(depends_on)}\n\n\ndef upgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n${upgrades if upgrades else \"    pass\"}\n    # ### END Alembic commands ###\n\n\ndef downgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n${downgrades if downgrades else \"    pass\"}",
    "alembic/versions/.gitkeep": ".",
    "backend/__init__.py": "\"\"\"\nInitializes the backend directory as a Python package.\n\"\"\"\n# This file can be left empty, or contain package-level initialization code.\n# For a simple FastAPI application, it often remains empty.\n# Any package-level imports or configurations can be added here if needed.",
    "backend/api/__init__.py": "\"\"\"\nInitializes the api directory as a Python package.\n\nThis file can be left empty, or contain package-level initialization code.\nFor a simple FastAPI application, it often remains empty, as routers are\ntypically imported and included in the main application instance.\n\"\"\"\n# Any package-level imports or configurations for the API can be added here if needed.\n# For example, if you had common API-wide middleware or dependencies that\n# applied to all API versions, they could be defined or imported here.",
    "backend/api/v1/__init__.py": "\"\"\"\nInitializes the v1 API version directory as a Python package.\n\nThis file can be left empty, or contain package-level initialization code\nspecific to API version 1. For a typical FastAPI application, it often\nremains empty, as routers are imported and included in `backend/api/v1/api.py`.\n\"\"\"\n# Any package-level imports or configurations for API v1 can be added here if needed.\n# For example, if there were common dependencies or middleware that applied\n# only to v1 endpoints, they could be defined or imported here.",
    "backend/api/v1/api.py": "from fastapi import APIRouter\n\nfrom backend.api.v1.endpoints import auth\nfrom backend.api.v1.endpoints import dashboard\nfrom backend.api.v1.endpoints import health\nfrom backend.api.v1.endpoints import market\nfrom backend.api.v1.endpoints import portfolio\nfrom backend.api.v1.endpoints import transactions\nfrom backend.api.v1.endpoints import users\nfrom backend.api.v1.endpoints import watchlist\n\n# Create the main API router for version 1\n# This router will aggregate all specific endpoint routers for API v1.\napi_router = APIRouter()\n\n# Include individual endpoint routers.\n# Each router is mounted with a specific prefix and tags for OpenAPI documentation.\n# The prefixes here are relative to the base path where `api_router` itself will be mounted\n# in the main FastAPI application (e.g., if `api_router` is mounted at `/api/v1`,\n# then `/health` here becomes `/api/v1/health`).\n\napi_router.include_router(health.router, prefix=\"/health\", tags=[\"Health\"])\napi_router.include_router(auth.router, prefix=\"/auth\", tags=[\"Authentication\"])\napi_router.include_router(users.router, prefix=\"/users\", tags=[\"Users\"])\napi_router.include_router(dashboard.router, prefix=\"/dashboard\", tags=[\"Dashboard\"])\napi_router.include_router(market.router, prefix=\"/market\", tags=[\"Market Data\"])\napi_router.include_router(portfolio.router, prefix=\"/portfolio\", tags=[\"Portfolio\"])\napi_router.include_router(transactions.router, prefix=\"/transactions\", tags=[\"Transactions\"])\napi_router.include_router(watchlist.router, prefix=\"/watchlist\", tags=[\"Watchlist\"])",
    "backend/api/v1/endpoints/__init__.py": "\"\"\"\nInitializes the endpoints directory as a Python package.\n\nThis file can be left empty, or contain package-level initialization code\nspecific to the API endpoints. For a typical FastAPI application, it often\nremains empty, as individual endpoint routers are imported and included\nin `backend/api/v1/api.py`.\n\"\"\"\n# Any package-level imports or configurations for the endpoints can be added here if needed.\n# For example, if there were common dependencies or middleware that applied\n# to all endpoints within this directory, they could be defined or imported here.",
    "backend/api/v1/endpoints/auth.py": "{user_in.email}\")\n        existing_user = await crud_user.get_by_email(db, email=user_in.email)\n        if existing_user:\n            logger.warning(f\"Registration failed: User with email '{user_in.email}' already exists.\")\n            raise DetailedHTTPException(\n                status_code=status.HTTP_400_BAD_REQUEST,\n                detail=\"The user with this email already exists in the system.\",\n                error_code=\"USER_ALREADY_EXISTS\"\n            )\n\n        hashed_password = get_password_hash(user_in.password)\n        user_create_data = user_in.model_dump()\n        user_create_data[\"hashed_password\"] = hashed_password\n        del user_create_data[\"password\"]  # Remove plain password before passing to CRUD\n\n        user = await crud_user.create(db, obj_in=user_create_data)\n        logger.info(f\"User registered successfully: {user.email}\")\n\n        # Optional: Add a background task for welcome email, logging, or other post-registration actions\n        # background_tasks.add_task(send_welcome_email, user.email)\n\n        return user\n    except DetailedHTTPException:\n        raise\n    except Exception as e:\n        logger.error(f\"Error during user registration for email '{user_in.email}': {e}\", exc_info=True)\n        raise DetailedHTTPException(\n            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n            detail=\"An unexpected error occurred during registration.\",\n            error_code=\"REGISTRATION_FAILED\"\n        )\n\n\n@router.post(\"/login\", response_model=Token)\nasync def login_for_access_token(\n    form_data: OAuth2PasswordRequestForm = Depends(),\n    db: AsyncSession = Depends(get_db)\n) -> Any:\n    \"\"\"\n    OAuth2 compatible token login. Authenticates a user and returns access and refresh tokens.\n\n    - **username**: User's email address.\n    - **password**: User's password.\n    \"\"\"\n    try:\n        user = await crud_user.get_by_email(db, email=form_data.username)\n        if not user or not verify_password(form_data.password, user.hashed_password):\n            logger.warning(f\"Failed login attempt for email: {form_data.username}\")\n            raise DetailedHTTPException(\n                status_code=status.HTTP_401_UNAUTHORIZED,\n                detail=\"Incorrect email or password\",\n                headers={\"WWW-Authenticate\": \"Bearer\"},\n                error_code=\"INVALID_CREDENTIALS\"\n            )\n\n        access_token_expires = timedelta(minutes=settings.ACCESS_TOKEN_EXPIRE_MINUTES)\n        refresh_token_expires = timedelta(days=settings.REFRESH_TOKEN_EXPIRE_DAYS)\n\n        access_token = create_access_token(\n            data={\"sub\": user.email, \"user_id\": str(user.id)}, expires_delta=access_token_expires\n        )\n        refresh_token = create_refresh_token(\n            data={\"sub\": user.email, \"user_id\": str(user.id)}, expires_delta=refresh_token_expires\n        )\n\n        logger.info(f\"User logged in successfully: {user.email}\")\n        return Token(access_token=access_token, refresh_token=refresh_token, token_type=\"bearer\")\n    except DetailedHTTPException:\n        raise\n    except Exception as e:\n        logger.error(f\"Error during user login for email '{form_data.username}': {e}\", exc_info=True)\n        raise DetailedHTTPException(\n            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n            detail=\"An unexpected error occurred during login.\",\n            error_code=\"LOGIN_FAILED\"\n        )\n\n\n@router.post(\"/refresh-token\", response_model=Token)\nasync def refresh_access_token(\n    token_refresh_request: TokenRefreshRequest,\n    db: AsyncSession = Depends(get_db)\n) -> Any:\n    \"\"\"\n    Refreshes an access token using a valid refresh token.\n    The refresh token itself is not renewed, only a new access token is issued.\n    \"\"\"\n    try:\n        payload = decode_token(token_refresh_request.refresh_token)\n        if payload is None:\n            logger.warning(\"Invalid refresh token provided or token expired.\")\n            raise DetailedHTTPException(\n                status_code=status.HTTP_401_UNAUTHORIZED,\n                detail=\"Invalid or expired refresh token\",\n                headers={\"WWW-Authenticate\": \"Bearer\"},\n                error_code=\"INVALID_REFRESH_TOKEN\"\n            )\n\n        user_id = payload.get(\"user_id\")\n        if not user_id:\n            logger.warning(\"Refresh token payload missing user_id.\")\n            raise DetailedHTTPException(\n                status_code=status.HTTP_401_UNAUTHORIZED,\n                detail=\"Invalid refresh token payload\",\n                headers={\"WWW-Authenticate\": \"Bearer\"},\n                error_code=\"INVALID_REFRESH_TOKEN_PAYLOAD\"\n            )\n\n        user = await crud_user.get(db, id=user_id)\n        if not user:\n            logger.warning(f\"User not found for refresh token with user_id: {user_id}\")\n            raise DetailedHTTPException(\n                status_code=status.HTTP_401_UNAUTHORIZED,\n                detail=\"User not found for refresh token\",\n                headers={\"WWW-Authenticate\": \"Bearer\"},\n                error_code=\"USER_NOT_FOUND_FOR_REFRESH\"\n            )\n\n        # Generate a new access token\n        access_token_expires = timedelta(minutes=settings.ACCESS_TOKEN_EXPIRE_MINUTES)\n        new_access_token = create_access_token(\n            data={\"sub\": user.email, \"user_id\": str(user.id)}, expires_delta=access_token_expires\n        )\n\n        logger.info(f\"Access token refreshed for user: {user.email}\")\n        return Token(access_token=new_access_token, refresh_token=token_refresh_request.refresh_token, token_type=\"bearer\")\n    except DetailedHTTPException:\n        raise\n    except Exception as e:\n        logger.error(f\"Error during token refresh: {e}\", exc_info=True)\n        raise DetailedHTTPException(\n            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n            detail=\"An unexpected error occurred during token refresh.\",\n            error_code=\"TOKEN_REFRESH_FAILED\"\n        )\n\n\n@router.get(\"/me\", response_model=UserResponse)\nasync def read_users_me(\n    current_user: UserResponse = Depends(get_current_active_user)\n) -> Any:\n    \"\"\"\n    Retrieve current authenticated user's details.\n    Requires a valid access token in the Authorization header.\n    \"\"\"\n    try:\n        logger.debug(f\"Fetching current user details for '{current_user.email}'\")\n        return current_user\n    except DetailedHTTPException:\n        raise\n    except Exception as e:\n        logger.error(f\"Error fetching current user details for '{current_user.email}': {e}",
    "backend/api/v1/endpoints/dashboard.py": "{current_user.email}\")\n\n        portfolio_assets = await crud_portfolio.get_multi_by_owner(db, owner_id=current_user.id)\n        if not portfolio_assets:\n            logger.info(f\"No portfolio assets found for user {current_user.email}. Returning empty summary.\")\n            return DashboardSummary(\n                total_portfolio_value=0.0,\n                total_cost_basis=0.0,\n                total_profit_loss=0.0,\n                total_profit_loss_percentage=0.0,\n                daily_change=0.0,\n                daily_change_percentage=0.0,\n            )\n\n        symbols = [asset.asset_symbol for asset in portfolio_assets]\n        current_prices = await market_data_service.get_current_prices(symbols)\n\n        total_portfolio_value = 0.0\n        total_cost_basis = 0.0\n        daily_change = 0.0\n        previous_day_value = 0.0\n\n        for asset in portfolio_assets:\n            current_price = current_prices.get(asset.asset_symbol)\n            if current_price is None:\n                logger.warning(f\"Current price not found for {asset.asset_symbol}. Skipping for summary calculation.\")\n                continue\n\n            asset_current_value = asset.quantity * current_price\n            total_portfolio_value += asset_current_value\n            total_cost_basis += asset.cost_basis\n\n            # Calculate daily change for each asset\n            # This requires fetching historical price for yesterday.\n            # For simplicity, we'll assume market_data_service can provide 24h change or yesterday's close.\n            # A more robust solution would fetch yesterday's close price explicitly.\n            yesterday_price = await market_data_service.get_historical_price(asset.asset_symbol, datetime.now() - timedelta(days=1))\n            if yesterday_price:\n                asset_previous_day_value = asset.quantity * yesterday_price\n                previous_day_value += asset_previous_day_value\n                daily_change += (current_price - yesterday_price) * asset.quantity\n            else:\n                logger.debug(f\"Yesterday's price not available for {asset.asset_symbol}. Daily change for this asset will be 0.\")\n\n        total_profit_loss = total_portfolio_value - total_cost_basis\n        total_profit_loss_percentage = (total_profit_loss / total_cost_basis * 100) if total_cost_basis > 0 else 0.0\n\n        daily_change_percentage = (daily_change / previous_day_value * 100) if previous_day_value > 0 else 0.0\n\n        summary = DashboardSummary(\n            total_portfolio_value=round(total_portfolio_value, 2),\n            total_cost_basis=round(total_cost_basis, 2),\n            total_profit_loss=round(total_profit_loss, 2),\n            total_profit_loss_percentage=round(total_profit_loss_percentage, 2),\n            daily_change=round(daily_change, 2),\n            daily_change_percentage=round(daily_change_percentage, 2),\n        )\n        logger.info(f\"Dashboard summary generated for user {current_user.email}.\")\n        return summary\n\n    except DetailedHTTPException:\n        raise\n    except Exception as e:\n        logger.error(f\"Error fetching dashboard summary for user {current_user.email}: {e}\", exc_info=True)\n        raise DetailedHTTPException(\n            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n            detail=\"An unexpected error occurred while retrieving dashboard summary.\",\n            error_code=\"DASHBOARD_SUMMARY_FAILED\",\n        )\n\n\n@router.get(\"/chart-data\", response_model=PortfolioChartData)\nasync def get_portfolio_chart_data(\n    timeframe: Timeframe = Timeframe.ONE_MONTH,\n    current_user: User = Depends(get_current_active_user),\n    db: AsyncSession = Depends(get_db),\n) -> Any:\n    \"\"\"\n    Retrieves historical portfolio value data for charting over a specified timeframe.\n    Accurately reconstructs holdings for each historical date.\n    \"\"\"\n    try:\n        logger.info(f\"Fetching portfolio chart data for user: {current_user.email}, timeframe: {timeframe.value}\")\n\n        end_date = datetime.now().date()\n        if timeframe == Timeframe.ONE_DAY:\n            start_date = end_date - timedelta(days=1)\n        elif timeframe == Timeframe.ONE_WEEK:\n            start_date = end_date - timedelta(weeks=1)\n        elif timeframe == Timeframe.ONE_MONTH:\n            start_date = end_date - timedelta(days=30)\n        elif timeframe == Timeframe.THREE_MONTHS:\n            start_date = end_date - timedelta(days=90)\n        elif timeframe == Timeframe.SIX_MONTHS:\n            start_date = end_date - timedelta(days=180)\n        elif timeframe == Timeframe.ONE_YEAR:\n            start_date = end_date - timedelta(days=365)\n        elif timeframe == Timeframe.FIVE_YEARS:\n            start_date = end_date - timedelta(days=5 * 365)\n        else:  # ALL\n            # Find the date of the first transaction\n            first_transaction = await crud_transaction.get_first_transaction_date_by_owner(db, owner_id=current_user.id)\n            start_date = first_transaction.date() if first_transaction else end_date - timedelta(days=30) # Default to 30 days if no transactions\n\n        # Ensure start_date is not in the future\n        if start_date > end_date:\n            start_date = end_date\n\n        # Get all transactions for the user within the relevant period\n        transactions = await crud_transaction.get_multi_by_owner(\n            db, owner_id=current_user.id, start_date=start_date, end_date=end_date\n        )\n        # Also get all transactions before the start_date to establish initial holdings\n        initial_transactions = await crud_transaction.get_multi_by_owner(\n            db, owner_id=current_user.id, end_date=start_date - timedelta(days=1)\n        )\n        all_transactions = sorted(initial_transactions + transactions, key=lambda t: t.transaction_date)\n\n        # Get all unique asset symbols involved in transactions\n        all_symbols = list(set(t.asset_symbol for t in all_transactions))\n        if not all_symbols:\n            logger.info(f\"No transactions found for user {current_user.email}. Returning empty chart data.\")\n            return PortfolioChartData(data=[], timeframe=timeframe)\n\n        # Fetch historical prices for all relevant symbols and the entire date range\n        historical_prices_data = await market_data_service.get_historical_prices_range(\n            symbols=all_symbols, start_date=start_date, end_date=end_date\n        )\n        # Reformat for easier lookup: {symbol: {date: price}}\n        historical_prices = {\n            symbol: {dp.date.date(): dp.close for dp in data_points}\n            for symbol, data_points in historical_prices_data.items()\n        }\n\n        chart_data_points: List[PortfolioChartDataPoint] = []\n        current_holdings = {}  # {symbol: quantity}\n        transaction_idx = 0\n\n        # Initialize holdings based on transactions before start_date\n        for t in all_transactions:\n            if t.transaction_date.date() < start_date:\n                if t.transaction_type == TransactionType.BUY:\n                    current_holdings[t.asset_symbol] = current_holdings.get(t.asset_symbol, 0) + t.quantity\n                elif t.transaction_type == TransactionType.SELL:\n                    current_holdings[t.asset_symbol] = current_holdings.get(t.asset_symbol, 0) - t.quantity\n            else:\n                break # Stop processing initial transactions\n\n        # Iterate day by day from start_date to end_date\n        current_date = start_date\n        while current_date <= end_date:\n            # Apply transactions that occurred on the current_date\n            while transaction_idx < len(all_transactions) and all_transactions[transaction_idx].transaction_date.date() == current_date:\n                transaction = all_transactions[transaction_idx]\n                if transaction.transaction_type == TransactionType.BUY:\n                    current_holdings[transaction.asset_symbol] = current_holdings.get(transaction.asset_symbol, 0) + transaction.quantity\n                elif transaction.transaction_type == TransactionType.SELL:\n                    current_holdings[transaction.asset_symbol] = current_holdings.get(transaction.asset_symbol, 0) - transaction.quantity\n                transaction_idx += 1\n\n            daily_portfolio_value = 0.0\n            for symbol, quantity in current_holdings.items():\n                if quantity > 0:\n                    price_on_date = historical_prices.get(symbol, {}).get(current_date)\n                    if price_on_date is not None:\n                        daily_portfolio_value += quantity * price_on_date\n                    else:\n                        logger.debug(f\"Historical price for {symbol} on {current_date} not found. Skipping for value calculation.\")\n\n            chart_data_points.append(PortfolioChartDataPoint(date=datetime.combine(current_date, datetime.min.time()), value=round(daily_portfolio_value, 2)))\n            current_date += timedelta(days=1)\n\n        logger.info(f\"Portfolio chart data generated for user {current_user.email} with {len(chart_data_points)} data points.\")\n        return PortfolioChartData(data=chart_data_points, timeframe=timeframe)\n\n    except DetailedHTTPException:\n        raise\n    except Exception as e:\n        logger.error(f\"Error fetching portfolio chart data for user {current_user.email}: {e}\", exc_info=True)\n        raise DetailedHTTPException(\n            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n            detail=\"An unexpected error occurred while retrieving portfolio chart data.\",\n            error_code=\"DASHBOARD_CHART_DATA_FAILED\",\n        )\n\n\n@router.get(\"/asset-distribution\", response_model=PortfolioAssetDistribution)\nasync def get_portfolio_asset_distribution(\n    current_user: User = Depends(get_current_active_user),\n    db: AsyncSession = Depends(get_db),\n) -> Any:\n    \"\"\"\n    Retrieves the distribution of assets in the user's portfolio,\n    categorized by asset type or sector.\n    \"\"\"\n    try:\n        logger.info(f\"Fetching portfolio asset distribution for user: {current_user.email}\")\n\n        portfolio_assets = await crud_portfolio.get_multi_by_owner(db, owner_id=current_user.id)\n        if not portfolio_assets:\n            logger.info(f\"No portfolio assets found for user {current_user.email}. Returning empty distribution.\")\n            return PortfolioAssetDistribution(distribution=[])\n\n        symbols = [asset.asset_symbol for asset in portfolio_assets]\n        current_prices = await market_data_service.get_current_prices(symbols)\n\n        total_portfolio_value = 0.0\n        asset_values = {} # {symbol: value}\n\n        for asset in portfolio_assets:\n            current_price = current_prices.get(asset.asset_symbol)\n            if current_price is None:\n                logger.warning(f\"Current price not found for {asset.asset_symbol}. Skipping for distribution calculation.\")\n                continue\n            asset_value = asset.quantity * current_price\n            asset_values[asset.asset_symbol] = asset_value\n            total_portfolio_value += asset_value\n\n        if total_portfolio_value == 0:\n            logger.info(f\"Total portfolio value is zero for user {current_user.email}. Returning empty distribution.\")\n            return PortfolioAssetDistribution(distribution=[])\n\n        # For distribution, we can categorize by asset type (e.g., Stock, Crypto)\n        # or by sector/industry if market_data_service provides this metadata.\n        # For now, let's use a simple asset type categorization.\n        # A more advanced implementation would fetch asset metadata.\n        distribution_map = {} # {category: total_value_in_category}\n\n        for asset in portfolio_assets:\n            asset_value = asset_values.get(asset.asset_symbol, 0.0)\n            if asset_value == 0:\n                continue\n\n            # Placeholder for actual asset type/sector lookup\n            # In a real system, market_data_service.get_asset_metadata(asset.asset_symbol)\n            # would provide 'type' (e.g., 'Stock', 'Crypto') or 'sector'.\n            # For this example, we'll infer a simple category.\n            category = \"Other\"\n            if asset.asset_symbol.isupper() and len(asset.asset_symbol) <= 5: # Simple heuristic for stocks\n                category = \"Stock\"\n            elif len(asset.asset_symbol) > 5 and asset.asset_symbol.isalnum(): # Simple heuristic for crypto\n                category = \"Cryptocurrency\"\n            # Add more sophisticated logic here if metadata is available\n\n            distribution_map[category] = distribution_map.get(category, 0.0) + asset_value\n\n        distribution_items: List[PortfolioAssetDistributionItem] = []\n        for category, value in distribution_map.items():\n            percentage = (value / total_portfolio_value) * 100\n            distribution_items.append(\n                PortfolioAssetDistributionItem(\n                    category=category,\n                    value=round(value, 2),\n                    percentage=round(percentage, 2),\n                )\n            )\n\n        # Sort by value descending\n        distribution_items.sort(key=lambda x: x.value, reverse=True)\n\n        logger.info(f\"Portfolio asset distribution generated for user {current_user.email}.\")\n        return PortfolioAssetDistribution(distribution=distribution_items)\n\n    except DetailedHTTPException:\n        raise\n    except Exception as e:\n        logger.error(f\"Error fetching portfolio asset distribution for user {current_user.email}: {e}",
    "backend/api/v1/endpoints/health.py": "{e}\", exc_info=True)\n        return HealthCheckDetail(\n            component=\"database\",\n            status=HealthStatus.DOWN,\n            message=f\"Database operational error: {e}\"\n        )\n    except SQLAlchemyError as e:\n        # Catch any other SQLAlchemy-related errors\n        logger.error(f\"Database SQLAlchemy error during health check: {e}\", exc_info=True)\n        return HealthCheckDetail(\n            component=\"database\",\n            status=HealthStatus.DOWN,\n            message=f\"Database connection failed: {e}\"\n        )\n    except Exception as e:\n        # Catch any other unexpected errors during the database check\n        logger.error(f\"An unexpected error occurred during database check: {e}\", exc_info=True)\n        return HealthCheckDetail(\n            component=\"database\",\n            status=HealthStatus.DOWN,\n            message=f\"Unexpected error during database check: {e}\"\n        )\n\ndef check_disk_usage() -> HealthCheckDetail:\n    \"\"\"\n    Checks the disk usage of the root partition where the application is running.\n    Compares current usage against a configured threshold.\n    Anticipates failures like `FileNotFoundError` or other `OSError`.\n    \"\"\"\n    try:\n        # Get disk usage for the current working directory's partition.\n        # This typically reflects the root partition in a containerized environment.\n        total, used, free = shutil.disk_usage(os.getcwd())\n        used_percent = (used / total) * 100 if total > 0 else 0\n\n        status_enum = HealthStatus.UP\n        message = (\n            f\"Disk usage: {used_percent:.2f}% \"\n            f\"(Total: {total / (1024**3):.2f} GB, Used: {used / (1024**3):.2f} GB)\"\n        )\n\n        if used_percent > settings.DISK_USAGE_THRESHOLD_PERCENT:\n            status_enum = HealthStatus.WARNING\n            message = (\n                f\"Disk usage is high: {used_percent:.2f}% exceeds threshold of \"\n                f\"{settings.DISK_USAGE_THRESHOLD_PERCENT}%.\"\n            )\n            logger.warning(message)\n\n        return HealthCheckDetail(\n            component=\"disk\",\n            status=status_enum,\n            message=message,\n            details={\n                \"used_percent\": round(used_percent, 2),\n                \"total_gb\": round(total / (1024**3), 2),\n                \"used_gb\": round(used / (1024**3), 2)\n            }\n        )\n    except FileNotFoundError:\n        logger.error(\"Disk usage check failed: Target path not found.\", exc_info=True)\n        return HealthCheckDetail(\n            component=\"disk\",\n            status=HealthStatus.DOWN,\n            message=\"Disk usage check failed: Target path not found.\"\n        )\n    except Exception as e:\n        logger.error(f\"Disk usage check failed: {e}\", exc_info=True)\n        return HealthCheckDetail(\n            component=\"disk\",\n            status=HealthStatus.DOWN,\n            message=f\"Disk usage check failed: {e}\"\n        )\n\ndef check_memory_usage() -> HealthCheckDetail:\n    \"\"\"\n    Checks the system's virtual memory usage.\n    Compares current usage against a configured threshold.\n    Requires the `psutil` library.\n    \"\"\"\n    try:\n        memory = psutil.virtual_memory()\n        used_percent = memory.percent\n\n        status_enum = HealthStatus.UP\n        message = (\n            f\"Memory usage: {used_percent:.2f}% \"\n            f\"(Total: {memory.total / (1024**3):.2f} GB, Used: {memory.used / (1024**3):.2f} GB)\"\n        )\n\n        if used_percent > settings.MEMORY_USAGE_THRESHOLD_PERCENT:\n            status_enum = HealthStatus.WARNING\n            message = (\n                f\"Memory usage is high: {used_percent:.2f}% exceeds threshold of \"\n                f\"{settings.MEMORY_USAGE_THRESHOLD_PERCENT}%.\"\n            )\n            logger.warning(message)\n\n        return HealthCheckDetail(\n            component=\"memory\",\n            status=status_enum,\n            message=message,\n            details={\n                \"used_percent\": round(used_percent, 2),\n                \"total_gb\": round(memory.total / (1024**3), 2),\n                \"used_gb\": round(memory.used / (1024**3), 2)\n            }\n        )\n    except Exception as e:\n        logger.error(f\"Memory usage check failed: {e}\", exc_info=True)\n        return HealthCheckDetail(\n            component=\"memory\",\n            status=HealthStatus.DOWN,\n            message=f\"Memory usage check failed: {e}\"\n        )\n\ndef check_cache_status() -> HealthCheckDetail:\n    \"\"\"\n    Placeholder for cache health check.\n    In a production environment, this would connect to an actual cache service\n    (e.g., Redis, Memcached) and perform a PING or a simple GET/SET operation\n    to verify connectivity and responsiveness.\n    For this project, assuming no external cache is critically configured yet,\n    or it's considered operational by default.\n    \"\"\"\n    # Example of how a real cache check might look (e.g., for Redis):\n    # try:\n    #     redis_client = get_redis_client() # Assuming a dependency for Redis client\n    #     redis_client.ping()\n    #     return HealthCheckDetail(component=\"cache\", status=HealthStatus.UP, message=\"Cache service operational.\")\n    # except Exception as e:\n    #     logger.error(f\"Cache health check failed: {e}\", exc_info=True)\n    #     return HealthCheckDetail(component=\"cache\", status=HealthStatus.DOWN, message=f\"Cache service failed: {e}\")\n\n    return HealthCheckDetail(\n        component=\"cache\",\n        status=HealthStatus.UP,  # Defaulting to UP as no external cache is explicitly configured\n        message=\"Cache service not configured or assumed operational.\"\n    )\n\n@router.get(\"/\", response_model=HealthCheckResponse, summary=\"Application Health Check\",\n            description=\"Returns the overall health status of the application, including uptime, version, and checks for database, disk, memory, and cache.\")\nasync def get_health_status(\n    db: AsyncSession = Depends(get_db_session)\n) -> HealthCheckResponse:\n    \"\"\"\n    Endpoint to provide a comprehensive health check of the application.\n\n    This endpoint aggregates the status of various critical components\n    (database, disk, memory, cache) and provides overall application\n    status, uptime, and version information.\n\n    Args:\n        db (AsyncSession): Asynchronous database session dependency.\n\n    Returns:\n        HealthCheckResponse: A Pydantic model containing the application's status,\n                             uptime, version, and detailed checks for various components.\n    \"\"\"\n    current_time = datetime.now(timezone.utc)\n    uptime_seconds = (current_time - _app_start_time).total_seconds()\n    uptime_str = str(timedelta(seconds=int(uptime_seconds)))\n\n    # Perform individual component checks.\n    # The database check is asynchronous, others are synchronous.\n    # For more complex async checks, asyncio.gather could be used to run them concurrently.\n    db_check = await check_database_status(db)\n    disk_check = check_disk_usage()\n    memory_check = check_memory_usage()\n    cache_check = check_cache_status()  # Placeholder for now\n\n    # Aggregate all component checks into a list\n    checks = [db_check, disk_check, memory_check, cache_check]\n\n    # Determine overall status based on individual component statuses\n    overall_status = HealthStatus.UP\n    for check in checks:\n        if check.status == HealthStatus.DOWN:\n            overall_status = HealthStatus.DOWN\n            break  # If any critical component is DOWN, the overall status is DOWN\n        elif check.status == HealthStatus.WARNING:\n            # If not already DOWN, a WARNING from any component makes the overall status WARNING\n            if overall_status == HealthStatus.UP:\n                overall_status = HealthStatus.WARNING\n\n    # Construct the final health check response\n    response = HealthCheckResponse(\n        status=overall_status,\n        uptime=uptime_str,\n        version=__version__,\n        timestamp=current_time,\n        checks=checks,\n        system_info={\n            \"os\": platform.system(),\n            \"os_version\": platform.release(),\n            \"python_version\": platform.python_version(),\n            \"processor\": platform.processor(),\n            \"architecture\": platform.machine(),\n        }\n    )\n\n    logger.info(f\"Health check performed. Overall status: {overall_status.value}",
    "backend/api/v1/endpoints/market.py": "{symbol}\",\n    response_model=MarketDataQuote,\n    summary=\"Get real-time market quote for a symbol\",\n    description=\"Fetches simulated real-time market data (quote) for a given asset symbol. Requires authentication.\",\n    status_code=status.HTTP_200_OK,\n    responses={\n        status.HTTP_404_NOT_FOUND: {\"model\": MarketDataError, \"description\": \"Symbol not found\"},\n        status.HTTP_500_INTERNAL_SERVER_ERROR: {\"model\": MarketDataError, \"description\": \"Market data service error\"},\n    },\n)\nasync def get_market_quote(\n    symbol: str = Query(..., description=\"The ticker symbol of the asset (e.g., 'AAPL', 'MSFT')\"),\n    current_user: User = Depends(get_current_active_user),\n    db: Session = Depends(get_db),  # db dependency is included for consistency, even if not directly used by market data service\n) -> MarketDataQuote:\n    \"\"\"\n    Retrieve the latest simulated market quote for a specified asset symbol.\n\n    This endpoint provides current price, open, high, low, volume, and change\n    information for a given stock or cryptocurrency symbol.\n    \"\"\"\n    logger.info(f\"User {current_user.email} requesting market quote for symbol: {symbol}\")\n    market_data_service = MarketDataService(\n        api_key=settings.MARKET_DATA_API_KEY,\n        base_url=settings.MARKET_DATA_BASE_URL,\n        provider=settings.MARKET_DATA_PROVIDER,\n    )\n    try:\n        quote = await market_data_service.get_quote(symbol)\n        if not quote:\n            logger.warning(f\"Market quote not found for symbol: {symbol}\")\n            raise HTTPException(\n                status_code=status.HTTP_404_NOT_FOUND,\n                detail=f\"Market quote for symbol '{symbol}' not found.\",\n            )\n        return quote\n    except HTTPException:\n        # Re-raise FastAPI HTTPExceptions directly\n        raise\n    except Exception as e:\n        logger.error(f\"Failed to fetch market quote for {symbol}: {e}\", exc_info=True)\n        raise HTTPException(\n            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n            detail=f\"Failed to fetch market quote for '{symbol}'. Please try again later.\",\n        )\n\n\n@router.get(\n    \"/history/{symbol}\",\n    response_model=List[MarketDataHistory],\n    summary=\"Get historical market data for a symbol\",\n    description=\"Fetches simulated historical market data (e.g., daily prices) for a given asset symbol. Requires authentication.\",\n    status_code=status.HTTP_200_OK,\n    responses={\n        status.HTTP_404_NOT_FOUND: {\"model\": MarketDataError, \"description\": \"Symbol not found or no historical data\"},\n        status.HTTP_500_INTERNAL_SERVER_ERROR: {\"model\": MarketDataError, \"description\": \"Market data service error\"},\n    },\n)\nasync def get_market_history(\n    symbol: str = Query(..., description=\"The ticker symbol of the asset\"),\n    interval: str = Query(\"1day\", description=\"Data interval (e.g., '1min', '1hour', '1day', '1week', '1month')\"),\n    start_date: Optional[str] = Query(None, description=\"Start date for historical data (YYYY-MM-DD)\"),\n    end_date: Optional[str] = Query(None, description=\"End date for historical data (YYYY-MM-DD)\"),\n    current_user: User = Depends(get_current_active_user),\n    db: Session = Depends(get_db),\n) -> List[MarketDataHistory]:\n    \"\"\"\n    Retrieve simulated historical market data for a specified asset symbol.\n\n    This endpoint provides open, high, low, close, and volume data over a specified\n    time interval and date range.\n    \"\"\"\n    logger.info(\n        f\"User {current_user.email} requesting historical data for symbol: {symbol}, interval: {interval}, \"\n        f\"start: {start_date}, end: {end_date}\"\n    )\n    market_data_service = MarketDataService(\n        api_key=settings.MARKET_DATA_API_KEY,\n        base_url=settings.MARKET_DATA_BASE_URL,\n        provider=settings.MARKET_DATA_PROVIDER,\n    )\n    try:\n        history = await market_data_service.get_historical_data(symbol, interval, start_date, end_date)\n        if not history:\n            logger.warning(\n                f\"No historical data found for symbol: {symbol} with interval {interval}, \"\n                f\"start: {start_date}, end: {end_date}\"\n            )\n            raise HTTPException(\n                status_code=status.HTTP_404_NOT_FOUND,\n                detail=f\"No historical data found for symbol '{symbol}' with the specified parameters.\",\n            )\n        return history\n    except HTTPException:\n        raise\n    except Exception as e:\n        logger.error(f\"Failed to fetch historical data for {symbol}: {e}\", exc_info=True)\n        raise HTTPException(\n            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n            detail=f\"Failed to fetch historical data for '{symbol}'. Please try again later.\",\n        )\n\n\n@router.get(\n    \"/search\",\n    response_model=List[MarketDataSearch],\n    summary=\"Search for assets\",\n    description=\"Searches for assets (stocks, cryptocurrencies, etc.) by keyword. Requires authentication.\",\n    status_code=status.HTTP_200_OK,\n    responses={\n        status.HTTP_500_INTERNAL_SERVER_ERROR: {\"model\": MarketDataError, \"description\": \"Market data service error\"},\n    },\n)\nasync def search_assets(\n    query: str = Query(..., min_length=1, description=\"Search query for asset name or symbol (e.g., 'Apple', 'BTC')\"),\n    current_user: User = Depends(get_current_active_user),\n    db: Session = Depends(get_db),\n) -> List[MarketDataSearch]:\n    \"\"\"\n    Search for assets based on a provided query string.\n\n    This endpoint allows users to find assets by their name or ticker symbol.\n    \"\"\"\n    logger.info(f\"User {current_user.email} searching for assets with query: {query}\")\n    market_data_service = MarketDataService(\n        api_key=settings.MARKET_DATA_API_KEY,\n        base_url=settings.MARKET_DATA_BASE_URL,\n        provider=settings.MARKET_DATA_PROVIDER,\n    )\n    try:\n        results = await market_data_service.search_assets(query)\n        return results\n    except Exception as e:\n        logger.error(f\"Failed to search assets for query '{query}': {e}\", exc_info=True)\n        raise HTTPException(\n            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n            detail=f\"Failed to search for assets with query '{query}'. Please try again later.\",\n        )\n\n\n@router.get(\n    \"/trending\",\n    response_model=List[TrendingAsset],\n    summary=\"Get trending assets\",\n    description=\"Fetches a simulated list of currently trending assets. Requires authentication.\",\n    status_code=status.HTTP_200_OK,\n    responses={\n        status.HTTP_500_INTERNAL_SERVER_ERROR: {\"model\": MarketDataError, \"description\": \"Market data service error\"},\n    },\n)\nasync def get_trending_assets(\n    current_user: User = Depends(get_current_active_user),\n    db: Session = Depends(get_db),\n) -> List[TrendingAsset]:\n    \"\"\"\n    Retrieve a simulated list of currently trending assets.\n\n    This endpoint provides a curated list of assets that are currently\n    showing significant market activity or interest.\n    \"\"\"\n    logger.info(f\"User {current_user.email} requesting trending assets.\")\n    market_data_service = MarketDataService(\n        api_key=settings.MARKET_DATA_API_KEY,\n        base_url=settings.MARKET_DATA_BASE_URL,\n        provider=settings.MARKET_DATA_PROVIDER,\n    )\n    try:\n        trending = await market_data_service.get_trending_assets()\n        return trending\n    except Exception as e:\n        logger.error(f\"Failed to fetch trending assets: {e}"
  },
  "metadata": {
    "main.py": {
      "chars": 12246,
      "hash": "d43f1d70f201e141",
      "reviews": 3,
      "written_at": "2026-02-15T09:29:24.321799"
    },
    ".dockerignore": {
      "chars": 362,
      "hash": "2a866f0caa40f3c4",
      "reviews": 1,
      "written_at": "2026-02-15T09:29:31.774374"
    },
    ".env.example": {
      "chars": 1520,
      "hash": "34585ab31decd3a5",
      "reviews": 1,
      "written_at": "2026-02-15T09:29:37.997766"
    },
    ".gitignore": {
      "chars": 767,
      "hash": "d6eeffc1c18a8ae4",
      "reviews": 1,
      "written_at": "2026-02-15T09:29:46.267312"
    },
    "Dockerfile": {
      "chars": 1512,
      "hash": "b19f254cfd9f835a",
      "reviews": 1,
      "written_at": "2026-02-15T09:29:53.333227"
    },
    "README.md": {
      "chars": 2623,
      "hash": "413fbee23b4e13a1",
      "reviews": 1,
      "written_at": "2026-02-15T09:30:13.047517"
    },
    "alembic.ini": {
      "chars": 1962,
      "hash": "1c996b12a1cb66df",
      "reviews": 1,
      "written_at": "2026-02-15T09:30:20.682950"
    },
    "alembic/env.py": {
      "chars": 3674,
      "hash": "3fe034958f871bbc",
      "reviews": 3,
      "written_at": "2026-02-15T09:31:41.313894"
    },
    "alembic/script.py.mako": {
      "chars": 640,
      "hash": "10fef854fac47254",
      "reviews": 1,
      "written_at": "2026-02-15T09:31:47.934710"
    },
    "alembic/versions/.gitkeep": {
      "chars": 1,
      "hash": "cdb4ee2aea69cc6a",
      "reviews": 1,
      "written_at": "2026-02-15T09:31:50.699215"
    },
    "backend/__init__.py": {
      "chars": 274,
      "hash": "d51ce9ef7072451e",
      "reviews": 1,
      "written_at": "2026-02-15T09:31:54.189368"
    },
    "backend/api/__init__.py": {
      "chars": 505,
      "hash": "ee0eb55a0906ee2a",
      "reviews": 1,
      "written_at": "2026-02-15T09:31:57.676531"
    },
    "backend/api/v1/__init__.py": {
      "chars": 521,
      "hash": "8d0f1b1ce04d51f3",
      "reviews": 1,
      "written_at": "2026-02-15T09:32:01.281732"
    },
    "backend/api/v1/api.py": {
      "chars": 1541,
      "hash": "66023e4d6f9f84fe",
      "reviews": 1,
      "written_at": "2026-02-15T09:32:09.983450"
    },
    "backend/api/v1/endpoints/__init__.py": {
      "chars": 565,
      "hash": "a1b68b36205ef6b9",
      "reviews": 1,
      "written_at": "2026-02-15T09:32:13.284244"
    },
    "backend/api/v1/endpoints/auth.py": {
      "chars": 6482,
      "hash": "960e0c47a0918057",
      "reviews": 3,
      "written_at": "2026-02-15T09:34:04.902901"
    },
    "backend/api/v1/endpoints/dashboard.py": {
      "chars": 13455,
      "hash": "7d53c06c0d76c519",
      "reviews": 3,
      "written_at": "2026-02-15T09:36:35.997106"
    },
    "backend/api/v1/endpoints/health.py": {
      "chars": 8280,
      "hash": "58e397377e173fd7",
      "reviews": 3,
      "written_at": "2026-02-15T09:39:18.712281"
    },
    "backend/api/v1/endpoints/market.py": {
      "chars": 7424,
      "hash": "65ec5c01907aedd3",
      "reviews": 3,
      "written_at": "2026-02-15T09:41:27.436170"
    }
  },
  "saved_at": "2026-02-15T09:41:27.436175"
}